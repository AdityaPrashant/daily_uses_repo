{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60fb54c9",
   "metadata": {},
   "source": [
    "# Using Word2Vec for creating clusters for text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37155193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\p_adi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\p_adi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.chdir(\"C:/Users/p_adi/OneDrive/Desktop/\")\n",
    "import random\n",
    "import string\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "398c2c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>3</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>No description yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>3</td>\n",
       "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
       "      <td>Razer</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>Target</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Leather Horse Statues</td>\n",
       "      <td>1</td>\n",
       "      <td>Home/Home DÃ©cor/Home DÃ©cor Accents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24K GOLD plated rose</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Jewelry/Necklaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Complete with certificate of authenticity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_id                                 name  item_condition_id  \\\n",
       "0         0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
       "1         1     Razer BlackWidow Chroma Keyboard                  3   \n",
       "2         2                       AVA-VIV Blouse                  1   \n",
       "3         3                Leather Horse Statues                  1   \n",
       "4         4                 24K GOLD plated rose                  1   \n",
       "\n",
       "                                       category_name brand_name  price  \\\n",
       "0                                  Men/Tops/T-shirts        NaN   10.0   \n",
       "1  Electronics/Computers & Tablets/Components & P...      Razer   52.0   \n",
       "2                        Women/Tops & Blouses/Blouse     Target   10.0   \n",
       "3               Home/Home DÃ©cor/Home DÃ©cor Accents        NaN   35.0   \n",
       "4                            Women/Jewelry/Necklaces        NaN   44.0   \n",
       "\n",
       "   shipping                                   item_description  \n",
       "0         1                                 No description yet  \n",
       "1         0  This keyboard is in great condition and works ...  \n",
       "2         1  Adorable top with a hint of lace and a key hol...  \n",
       "3         1  New with tags. Leather horses. Retail for [rm]...  \n",
       "4         0          Complete with certificate of authenticity  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_excel(\"new_data1.xlsx\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67402be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No description yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Complete with certificate of authenticity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shipping                                   item_description\n",
       "0         1                                 No description yet\n",
       "1         0  This keyboard is in great condition and works ...\n",
       "2         1  Adorable top with a hint of lace and a key hol...\n",
       "3         1  New with tags. Leather horses. Retail for [rm]...\n",
       "4         0          Complete with certificate of authenticity"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df1 = train_df[['shipping','item_description']]\n",
    "train_df1 = train_df1.iloc[0:10000,]\n",
    "train_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83af6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "def clean_text(text, tokenizer, stopwords):\n",
    "    \"\"\"Pre-process text and generate tokens\n",
    "\n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        Tokenized text.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \"\", text\n",
    "    )  # Remove punctuation\n",
    "\n",
    "    tokens = tokenizer(text)  # Get tokens from text\n",
    "    tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n",
    "    tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b24e0ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly picked stopwords for data cleaning\n",
    "\n",
    "s_words = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'also', 'am', 'an', 'and',\n",
    "              'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below',\n",
    "              'between', 'both', 'but', 'by', 'can', \"can't\", 'cannot', 'com', 'could', \"couldn't\", 'did',\n",
    "              \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'else', 'ever',\n",
    "              'few', 'for', 'from', 'further', 'get', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having',\n",
    "              'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "              \"how's\", 'however', 'http', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it',\n",
    "              \"it's\", 'its', 'itself', 'just', 'k', \"let's\", 'like', 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n",
    "              'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'otherwise', 'ought', 'our', 'ours',\n",
    "              'ourselves', 'out', 'over', 'own', 'r', 'same', 'shall', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\",\n",
    "              'should', \"shouldn't\", 'since', 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs',\n",
    "              'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\",\n",
    "              \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\",\n",
    "              'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
    "              \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\",\n",
    "              'www', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves','something',\n",
    "              'biden',\"https\",'about','obama',\" ' \", 'today','years','history','trumptards','trump','nsauagv','president',\n",
    "             'everyone','someone','somebody','gonna','wanna','nprobably','calling','vsdvx','first','right','early','think',\n",
    "             'donald','applicati','country','people','watch','start','still','dance','thank','pennsylvania',\n",
    "             'applications','doesn','coming','going','hillary','getting','nblame','government','aaaaaarrrrrrrrrggggghhhh',\n",
    "              'aaand','aable','aaderwdjcw','aaron','aasohahvzm','aatmjzokf','aaufysks','aavyeeq','abandon','abandoned',\n",
    "              'abeyance','abiding','abilities','ability','abisv','abjrnaobrl','ablaz','ablcqv','ableist','abmst',\n",
    "              'abnormal','abolish','aborted','aborti','aborting','media','gkzmbxwljn','glffq','ntarifyg','gkrqf',\n",
    "              'gksaxfrsrv','gktmbs','glcrknprol','applica','rudygiuliani','realdonaldtrump','steveschmidtses',\n",
    "              'projectlincoln','guttenberg','unroll','called','mentioned','office','marine','party','anything','thing',\n",
    "              'never','jbyqgrdyt','kvmj','ucvx','nnsf','kvcvhmjxur','jgegxuj','gumgbvxxa','tupoqzxsfb','yqomrt','will',\n",
    "             'oann','hkrassenstein','trying','cthe','made','gtconway','zzzzzzzzzzzzz','ryannmcenany','ghiorth','giannocaldwell'\n",
    "             ,'giancarloc','ghtsfkcidm','brianclowdus','senatemajldr','mcconnell','caslernoel','zzjkevi','california','vxmeiljljf'\n",
    "              ,'youve','gflf','gina','mariabartiromo','chuckgrassley','zzfjzcfk','gigagrouch','itsjefftiedrich','covid',\n",
    "             'coylvukyhg','tachaprays','josprotests','josprotes','naija','endinsecurity','queernigerianlivesmatter','endsars'\n",
    "             'gcmfr','merisanjeevni','agrasen','rhviypiswo','arup','silchar','selombang','gbvyexasah','australianfederalgovernment',\n",
    "              'ausvotes','noahlindquist','singerinkansas','lindquist','jacinda','couvrefeu','drericding','dlqsqmjyln',\n",
    "              'lmkdasjhqw','ckyor','gdvqnhagee','gcmaf','gdzie','gcpmfoukpt','gcpiuhcd','etwfo','durgapuja','gbvyexasah',\n",
    "              'gbxsbkokwi','gbvfy','qmart']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bba1a94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe: (60614, 2)\n",
      "Pre-processed dataframe: (53846, 2)\n"
     ]
    }
   ],
   "source": [
    "custom_stopwords = set(stopwords.words(\"english\") + s_words)\n",
    "text_columns = [\"item_description\"]\n",
    "\n",
    "df = train_df1.copy()\n",
    "df[\"item_description\"] = df[\"item_description\"].fillna(\"\")\n",
    "\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "# Create text column based on title, description, and content\n",
    "df[\"text\"] = df[text_columns].apply(lambda x: \" | \".join(x), axis=1)\n",
    "df[\"tokens\"] = df[\"text\"].map(lambda x: clean_text(x, word_tokenize, custom_stopwords))\n",
    "\n",
    "# Remove duplicated after preprocessing\n",
    "_, idx = np.unique(df[\"tokens\"], return_index=True)\n",
    "df = df.iloc[idx, :]\n",
    "\n",
    "# Remove empty values and keep relevant columns\n",
    "df = df.loc[df.tokens.map(lambda x: len(x) > 0), [\"text\", \"tokens\"]]\n",
    "\n",
    "docs = df[\"text\"].values\n",
    "tokenized_docs = df[\"tokens\"].values\n",
    "\n",
    "print(f\"Original dataframe: {train_df1.shape}\")\n",
    "print(f\"Pre-processed dataframe: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4e1ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=tokenized_docs, vector_size=100, workers=3, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6607b410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53846, 100)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features\n",
    "    \n",
    "vectorized_docs = vectorize(tokenized_docs, model=model)\n",
    "len(vectorized_docs), len(vectorized_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "596b933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mbkmeans_clusters(X, k, mb, print_silhouette_values,):\n",
    "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a79a5f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 10\n",
      "Silhouette coefficient: 0.09\n",
      "Inertia:423080.8666753484\n",
      "Silhouette values:\n",
      "    Cluster 8: Size:1887 | Avg:0.14 | Min:-0.03 | Max: 0.35\n",
      "    Cluster 1: Size:5417 | Avg:0.12 | Min:-0.05 | Max: 0.31\n",
      "    Cluster 2: Size:8061 | Avg:0.12 | Min:0.01 | Max: 0.29\n",
      "    Cluster 7: Size:5910 | Avg:0.11 | Min:-0.05 | Max: 0.32\n",
      "    Cluster 4: Size:6149 | Avg:0.09 | Min:-0.04 | Max: 0.28\n",
      "    Cluster 5: Size:7043 | Avg:0.07 | Min:-0.10 | Max: 0.29\n",
      "    Cluster 9: Size:6156 | Avg:0.06 | Min:-0.06 | Max: 0.22\n",
      "    Cluster 0: Size:4822 | Avg:0.06 | Min:-0.07 | Max: 0.23\n",
      "    Cluster 6: Size:4705 | Avg:0.05 | Min:-0.07 | Max: 0.25\n",
      "    Cluster 3: Size:3696 | Avg:0.05 | Min:-0.12 | Max: 0.28\n"
     ]
    }
   ],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusters(X=vectorized_docs, k=10, mb=500, print_silhouette_values=True)\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": docs,\n",
    "    \"tokens\": [\" \".join(text) for text in tokenized_docs],\n",
    "    \"cluster\": cluster_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e58281a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most representative terms per cluster (based on centroids):\n",
      "Cluster 0: striped plaid stripe maroon sweater \n",
      "Cluster 1: hoola paaarty bareminerals foundations younique \n",
      "Cluster 2: jewels jeweled tulle rows stacked \n",
      "Cluster 3: deadstock newnever thiss yeah og \n",
      "Cluster 4: disk disks dslr functionality paperwork \n",
      "Cluster 5: preowned wrinkled preloved flaws piling \n",
      "Cluster 6: juniors xxl oversized xsmall runs \n",
      "Cluster 7: lowballs pls stated offered resale \n",
      "Cluster 8: pet freepet wrinkly wornwashed worthington \n",
      "Cluster 9: xsmall med xhilaration nwot xss \n"
     ]
    }
   ],
   "source": [
    "print(\"Most representative terms per cluster (based on centroids):\")\n",
    "for i in range(10):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=5)\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65d0d8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNIB, never used Color: Medium Includes: - deluxe deep dive cleansing gel - deluxe Rainforest of the Seaâ„¢ 4-in-1 setting mist - deluxe drink of H2O hydrating boost moisturizer - deluxe BB tinted treatment primer SPF 30 in Light - deluxe lights, camera, lashesâ„¢ 4-in-1 mascara - deluxe Amazonian clay 12-hour blush in thrilled (dusty mauve) SOLD OUT ONLINE âœˆï¸FREE SHIPPING â€¼ï¸Firm price â­ï¸Bundle & save Thanks for lookingâ—ï¸\n",
      "-------------\n",
      "12 beauty items plus metallic gold cosmetics bag. All products are new and unused. Combination of sample size and full size. Broad range of mid-high end brands. Includes: Trust Fund Beauty nail polish ([rm] latte), NYX lip butter (lifeguard), GRLPWR liquid lipstick, TheBalm Bahama Mama bronzer, Lancome Bi-facial, Tokyomilk Dark perfume, Clean perfume (cotton t-shirt), Kerastase Cristal hair mask, Sephora waterproof eye makeup remover, Sephora cheek ink gel, and CoverFX custom enhancer drops. Plus i'll throw in some extra perfume samples. Great value!\n",
      "-------------\n",
      "ON HOLD Tarte double duty contour Concealer!! Color is 'light sand' Brand new, authentic, and never used!! Swatches once! Bundle for more deals! Made with: Shea butter Sunflower Peony Licorice root Vitamin E Animal cruelty free 10 ml\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "## Most representative cluster in a particular cluster\n",
    "test_cluster = 1\n",
    "most_representative_docs = np.argsort(\n",
    "    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n",
    ")\n",
    "for d in most_representative_docs[:3]:\n",
    "    print(docs[d])\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b97931e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853dda69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
